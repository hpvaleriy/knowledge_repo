{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: German Hot Lead Detection\n",
    "authors:\n",
    "- sai_dunoyer \n",
    "tags:\n",
    "- cr3\n",
    "- prediction\n",
    "- hot\n",
    "- lead\n",
    "- scoring\n",
    "- questionnaire\n",
    "\n",
    "created_at: 2018-07-31\n",
    "updated_at: 2018-07-31\n",
    "tldr: This is a script used for the predict the potential purchase of a lead. \n",
    "goals: \n",
    "- improve the CR2*CR3 by changing the way we interact with publishers (firing pixel only if the lead is a hot lead)\n",
    "- identify a hot lead right after it is generated\n",
    "stakeholders:\n",
    "- Marco Wiesmann (DE Online Marketing)\n",
    "- Jeongmin Lee (DE Online Marketing + BI)\n",
    "\n",
    "scope: \n",
    "- lead.country = DE\n",
    "- lead.email_only = false\n",
    "- lead.source = 'Web2Lead'\n",
    "- campaign.controlling_channel != 'CRM'\n",
    "- lead.created_date >= '2017-01-01'\n",
    "- lead.status = 'qualifier' or 'closed'\n",
    "- opportunity.stage_name = 'Closed and Won' or 'Closed and Lost'\n",
    "- only leads that are [qualified & Closed and Won] or [qualified & Closed and Lost] or closed\n",
    "- no inbound calls (time_to_first_call null)\n",
    "\n",
    "\n",
    "approach: \n",
    "- identify the leads that are more likely to buy a hearing aid\n",
    "- t-test\n",
    "- decision trees\n",
    "- machine learning (classification algorithms)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Findings*\n",
    "\n",
    " - With the scope of data until 30.03.2018, the best algorithms could find hot subgroups with a 55% uplift of the success rate\n",
    " - The explicit algorithms should reach an uplift of 40%\n",
    "\n",
    "*Decision*\n",
    " - The DE Online Marketing Team wants first to filter the leads that are auto-closed and fire the pixel only in the case of non auto closed lead.\n",
    " - 31.07.2018: It had not yet been implemented (Jeongmin Lee is responsible) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: optimize in this section for **context setting**, as specifically as you can. For instance, this post is generally a set of standards for work in the repo. The specific motivation is to have least friction to current workflow while being able to painlessly aggregate it later.*\n",
    "\n",
    "The knowledge repo was created to consolidate research work that is currently scattered in emails, blogposts, and presentations, so that people didn't redo their work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The script used for this analysis\n",
    "Nota Bene: The script won't work if you don't have access to the talend datawarehouse database. If you have access, you should execute:\n",
    "keyring.set_password(\"talend\", \"sdunoyer\", \"my-password\") with sdunoyer replaced by your username and my-password by your password. You only have to execute that once and the set of username + password will be saved safely on your computer.\n",
    "When you execute: keyring.get_password('talend','sdunoyer') it will fetch the password safely stored on your computer. Replace in the script every sdunoyer by your username.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import keyring #store passwords locally and securely using keyring \n",
    "#keyring.set_password(\"mail\", \"your-mail-address\", \"your-mail-pw\")\n",
    "#keyring.set_password(\"talend\", \"sdunoyer\", \"my-password\")\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "pd.set_option('display.max_rows', 450)\n",
    "pd.set_option('display.max_columns', 400)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode() # run at the start of every ipython notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Data: Import Audibene German Web Leads \n",
    "Filters applied: \n",
    "    - lead.country = DE\n",
    "    - lead.email_only = false\n",
    "    - lead.source = 'Web2Lead'\n",
    "    - campaign.controlling_channel != 'CRM'\n",
    "    - lead.created_date >= '2017-01-01'\n",
    "    - lead.status = 'qualified' or 'closed'\n",
    "    - opportunity.stage_name = 'Closed and Won' or 'Closed and Lost'\n",
    "    - only leads that are [qualified & closed and won] OR [qualified & closed and lost] OR closed\n",
    "    - no time_to_first_call null (means inbound calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "con = psycopg2.connect(dbname='talend', port = '6432', user='sdunoyer', host='bi-proxy', password=keyring.get_password('talend','sdunoyer'))\n",
    "cur = con.cursor()\n",
    "\n",
    "# ---- pull Audibene German Weads ---- #\n",
    "cur.execute('''select l.id as lead_id, \n",
    "l.status as lead_status, \n",
    "l.created_date as lead_created_date, \n",
    "l.reached_date as lead_reached_date, \n",
    "l.reason_for_closing, \n",
    "l.number_of_unsuccessful_attempts,\n",
    "q.precise_age,\n",
    "l.created_during_office_hours, l.time_to_first_call, l.user_device, l.t_parameter, l.sub_publisher,\n",
    "a.usage, a.marketing_partner, a.controlling_channel as act_controlling_channel, a.offer_type, a.dw_created_at, a.dw_modified_at,\n",
    "o.stage_name,\n",
    "q.cardiac_pacemaker, q.cosi_basic_subtype_1, q.age_of_current_hearing_aid, q.current_hearing_test,\n",
    "q.degree_of_suffering, q.discreet_design, q.income_group, q.insurance_type, q.postal_code, q.prescription,\n",
    "q.purchase_timeframe, q.searching_for, q.tinnitus, q.type_of_treatment,\n",
    "q.willing_to_invest, q.why_not_sooner, q.salutation, q.email_filled, q.alternative_phone_filled, q.manufacturer_of_current_hearing_aid,\n",
    "q.satisfaction_current_device, q.professional_status, q.willing_to_invest_time, q.browser, q.operating_system,\n",
    "q.currently_looking_for_hearing_aids, q.marketing_offer\n",
    "from datamart.dim_lead l\n",
    "       left join datamart.dim_opportunity o on l.id = o.lead_id\n",
    "       left join datamart.dim_act a on l.act = a.act and l.country_code_iso3 = a.country_code_iso3\n",
    "       left join datamart.dim_questionnaire_dmk q on q.lead_id = l.id\n",
    "where l.country_code_iso3 = 'DEU'  and l.source = 'Web2Lead'\n",
    "       and a.controlling_channel != 'CRM' and l.created_date >= '2017-01-01' and l.status in ('qualified','closed')\n",
    "''')\n",
    "rows = cur.fetchall()\n",
    "leads = pd.DataFrame(rows, columns =  [elt[0] for elt in cur.description])\n",
    "print (leads.shape)\n",
    "cur.close()\n",
    "con.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "leads.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leads_backup = leads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleanup and Transform the Data\n",
    "## 2.1. Data Cleaning\n",
    "### 2.1.1. Remove rows by filtering leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Filter 1: Keep only stage_name in (closed and lost, closed and won or NA)\n",
    "leads = leads[(leads['stage_name']=='closed and lost')|(leads['stage_name']=='closed and won')|leads['stage_name'].isnull()]\n",
    "#Filter 2: Keep only closed or qualified & lost or qualified & won\n",
    "leads = leads[(leads['lead_status']=='closed') | ((leads['lead_status']=='qualified')&(leads['stage_name'].notnull()==True))].reset_index()\n",
    "print (leads.shape)\n",
    "#Filter 3: keep only leads with positive time_to_first_call\n",
    "leads = leads[leads['time_to_first_call'].isnull()==False]\n",
    "print (leads.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Improve Data Quality: replace german fields by english "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leads.cardiac_pacemaker = np.where(leads.cardiac_pacemaker == 'nein', 'no', leads.cardiac_pacemaker)\n",
    "leads.current_hearing_test = np.where(leads.current_hearing_test == 'nein', 'no', leads.current_hearing_test)\n",
    "leads.current_hearing_test = np.where(leads.current_hearing_test == 'ja', 'yes', leads.current_hearing_test)\n",
    "leads.discreet_design = np.where(leads.discreet_design == 'sehr wichtig', 'very important', leads.discreet_design)\n",
    "leads.insurance_type = np.where(leads.insurance_type == 'gesetzlich', 'statutory', leads.insurance_type)\n",
    "leads.salutation = np.where(leads.salutation == 'herr', 'mr.', leads.salutation)\n",
    "leads.salutation = np.where(leads.salutation == 'ms.', 'mrs.', leads.salutation)\n",
    "leads.operating_system = np.where((leads.operating_system == 'macos') | \n",
    "                                  (leads.operating_system == 'os x') |\n",
    "                                  (leads.operating_system == 'iphone os') |\n",
    "                                  (leads.operating_system == 'mac os x') |\n",
    "                                  (leads.operating_system == 'mac')\n",
    "                                  , 'macos/ios', leads.operating_system)\n",
    "leads.operating_system = np.where((leads.operating_system == 'android') | \n",
    "                                  (leads.operating_system == 'linux') |\n",
    "                                  (leads.operating_system == 'fireos') |\n",
    "                                  (leads.operating_system == 'fedora') |\n",
    "                                  (leads.operating_system == 'tizen') |\n",
    "                                  (leads.operating_system == 'android tv') |\n",
    "                                  (leads.operating_system == 'webos') |\n",
    "                                  (leads.operating_system == 'chrome os')\n",
    "                                  , 'ubuntu', leads.operating_system)\n",
    "leads.operating_system = np.where(leads.operating_system == 'windows phone'\n",
    "                                  , 'windows', leads.operating_system)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Data Derivation : create new variables :\n",
    "    - outcome variable: purchase\n",
    "    - day_of_week\n",
    "    - weekday_or_weekend\n",
    "    - completed_time\n",
    "    - completed_time_segment\n",
    "    - hour of day\n",
    "    - questionnaire_fill_up_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import time, tzinfo, timedelta\n",
    "#create outcome variable: purchase\n",
    "leads['purchase'] = np.where(leads['lead_status'] == 'closed', \n",
    "                             0, \n",
    "                             np.where(leads['stage_name']== 'closed and won', 1, 0))\n",
    "#time and date derivation\n",
    "leads['day_of_week'] = leads.lead_created_date.dt.weekday + 1\n",
    "leads['weekday_or_weekend'] = np.where(leads.day_of_week >=6, 'weekend', 'weekday')\n",
    "leads['completed_time'] = leads.lead_created_date.dt.time\n",
    "leads['completed_time_segment'] = np.where(leads.completed_time < time(6, 0, 0),\n",
    "                                           \"0-6\",\n",
    "                                           np.where(leads.completed_time < time(8, 0, 0),\n",
    "                                                   \"6-8\",\n",
    "                                                   np.where(leads.completed_time < time(10, 0, 0),\n",
    "                                                           \"8-10\",\n",
    "                                                           np.where(leads.completed_time < time(12, 0, 0),\n",
    "                                                                   \"10-12\",\n",
    "                                                                   np.where(leads.completed_time < time(14, 0, 0),\n",
    "                                                                           \"12-14\",\n",
    "                                                                           np.where(leads.completed_time < time(16, 0, 0),\n",
    "                                                                                   \"14-16\",\n",
    "                                                                                   np.where(leads.completed_time < time(18, 0, 0),\n",
    "                                                                                           \"16-18\",\n",
    "                                                                                           np.where(leads.completed_time < time(20, 0, 0),\n",
    "                                                                                                   \"18-20\",\n",
    "                                                                                                   \"20-24\"))))))))\n",
    "leads['hour_of_day'] = leads.lead_created_date.dt.hour\n",
    "\n",
    "#age bucket\n",
    "leads['age_bucket'] = np.where(leads.precise_age<62, \n",
    "                                        '<62', \n",
    "                                        np.where(leads.precise_age <69, \n",
    "                                                 '62-68', np.where(leads.precise_age < 75, '69-75', \n",
    "                                                                  np.where(leads.precise_age < 81, '76-80', '>80'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions_first_care = ['precise_age', # compulsory\n",
    "                        'cosi_basic_subtype_1', \n",
    "                        'current_hearing_test',\n",
    "                        'degree_of_suffering',\n",
    "                        'discreet_design',\n",
    "                        'insurance_type',\n",
    "                        'postal_code', # compulsory since CW18 2017 (4’th may)\n",
    "                        'prescription',\n",
    "                        'purchase_timeframe',\n",
    "                        'searching_for',\n",
    "                        'tinnitus',\n",
    "                        'type_of_treatment',\n",
    "                        'why_not_sooner',\n",
    "                        'salutation',# compulsory\n",
    "                        'email_filled',# compulsory\n",
    "                        'professional_status'] #16\n",
    "\n",
    "questions_follow_up_care = ['precise_age',# compulsory\n",
    "                            'cosi_basic_subtype_1',\n",
    "                            'degree_of_suffering',\n",
    "                            'discreet_design',\n",
    "                            'insurance_type',\n",
    "                            'postal_code',# compulsory  since CW18 2017 (4’th may)\n",
    "                            'purchase_timeframe',\n",
    "                            'searching_for',\n",
    "                            'tinnitus',\n",
    "                            'type_of_treatment',\n",
    "                            'salutation',# compulsory\n",
    "                            'email_filled',# compulsory\n",
    "                            'professional_status'] #13\n",
    "\n",
    "leads['questionnaire_fill_up_rate'] = np.where(leads.type_of_treatment == 'first care', \n",
    "                                               leads[questions_first_care].apply(lambda x: x.count(), axis=1)/16,\n",
    "                                               leads[questions_follow_up_care].apply(lambda x: x.count(), axis=1)/13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('The dataset contains ', leads.shape[0], ' closed leads and ', leads.shape[1], 'features.')\n",
    "print('{0:.2f}%'.format((leads.purchase.value_counts()[1]/leads.shape[0] * 100)), ' of the closed leads have purchased a Hearing Aid through Audibene.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Data Transformation:\n",
    "    - transform categorical variables to 1/0 variables\n",
    "    - exclude some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#columns included for the analysis\n",
    "categorical_columns = [\n",
    "    'user_device', \n",
    "    'cosi_basic_subtype_1',\n",
    "    'age_of_current_hearing_aid', \n",
    "    'current_hearing_test',\n",
    "    'degree_of_suffering', \n",
    "    'discreet_design', \n",
    "    'insurance_type', \n",
    "    'prescription', \n",
    "    'purchase_timeframe',\n",
    "    'searching_for', \n",
    "    'tinnitus', \n",
    "    'type_of_treatment',\n",
    "    'why_not_sooner', \n",
    "    'salutation',\n",
    "    'manufacturer_of_current_hearing_aid',\n",
    "    'satisfaction_current_device',\n",
    "    'professional_status', \n",
    "    'browser',\n",
    "    'operating_system',\n",
    "    'weekday_or_weekend',\n",
    "    'completed_time_segment', \n",
    "    'age_bucket']\n",
    "\n",
    "#create dummies: transform categorical variables into 0/1 variables\n",
    "dummies = pd.get_dummies((leads[categorical_columns]))\n",
    "#join dummies \n",
    "leads = pd.concat([leads, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Columns filtering \n",
    "        - remove IDs\n",
    "        - remove columns related to outcome\n",
    "        - remove columns related to marketing partner or offer\n",
    "        - remove dates\n",
    "        - remove columns that have under 5% of fill-up rate or too granular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove some columns \n",
    "columns_to_drop = [\n",
    "    'lead_id', #id\n",
    "    'lead_status', #related to outcome variable\n",
    "    'lead_created_date', #date\n",
    "    'lead_reached_date', #date\n",
    "    'completed_time', #time\n",
    "    'reason_for_closing', #after lead is generated\n",
    "    'time_to_first_call', #after lead is generated\n",
    "    'number_of_unsuccessful_attempts', #after lead is generated\n",
    "    't_parameter', #A/B testing parameters, not relevant\n",
    "    'sub_publisher', #too granular, try to segment in categories\n",
    "    'usage', #too granular, try to segment in categories\n",
    "    'dw_created_at', #date\n",
    "    'dw_modified_at', #date\n",
    "    'stage_name',#related to outcome variable\n",
    "    'postal_code', #too granular, segmented in region and east/west,\n",
    "    'index',\n",
    "    'act_controlling_channel', \n",
    "    'marketing_partner',\n",
    "    'offer_type',\n",
    "    'marketing_offer'\n",
    "]\n",
    "\n",
    "#remove low fill-up rates\n",
    "fill_up = pd.DataFrame((leads.shape[0] - leads.isnull().sum())/leads.shape[0])\n",
    "low_fill_up_features = fill_up[fill_up[0]<0.05].index\n",
    "\n",
    "for i in range(len(low_fill_up_features)): \n",
    "    columns_to_drop.append(low_fill_up_features[i]) \n",
    "\n",
    "#drop categorical columns that were transformed to dummies\n",
    "columns_to_drop.append(['user_device',\n",
    " 'cosi_basic_subtype_1',\n",
    " 'age_of_current_hearing_aid',\n",
    " 'current_hearing_test',\n",
    " 'degree_of_suffering',\n",
    " 'discreet_design',\n",
    " 'insurance_type',\n",
    " 'prescription',\n",
    " 'purchase_timeframe',\n",
    " 'searching_for',\n",
    " 'tinnitus',\n",
    " 'type_of_treatment',\n",
    " 'why_not_sooner',\n",
    " 'salutation',\n",
    " 'manufacturer_of_current_hearing_aid',\n",
    " 'satisfaction_current_device',\n",
    " 'professional_status',\n",
    " 'browser',\n",
    " 'operating_system',\n",
    " 'weekday_or_weekend',\n",
    " 'completed_time_segment',\n",
    " 'age_bucket'])\n",
    "  \n",
    "leads = leads.drop(columns_to_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Success Rate accross all dimensions (1D Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#statistical test : Student T test. Test if success_rate is same accross groups.\n",
    "# A large t-score tells you that the groups are different.\n",
    "# A small t-score tells you that the groups are similar.\n",
    "# Null hypothesis: SR accross groups are same\n",
    "# Alternative hypothesis: SR accross groups are different\n",
    "from scipy import stats\n",
    "import scipy.stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "for col in dummies:\n",
    "    result_ttest = stats.ttest_ind(leads[leads['purchase']==1][col], leads[leads['purchase']==0][col])\n",
    "    if(result_ttest[1]<0.05):\n",
    "        print(col)\n",
    "        print(result_ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Construct Datasets (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = leads\n",
    "X = X.drop('purchase', axis = 1)\n",
    "y = leads.purchase\n",
    "features=X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test train & validation split\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "#train = 70%\n",
    "#test = 30% * 70* = 21%\n",
    "#validation = 30% * 30$ = 9%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=3, \n",
    "                                                    test_size=0.30)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, \n",
    "                                                y_test, \n",
    "                                                test_size=0.3, \n",
    "                                                random_state=3)\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "print('X_val:', X_val.shape)\n",
    "print('y_val:', y_val.shape)\n",
    "\n",
    "print('Train SR: ', y_train.sum()/y_train.count())\n",
    "print('Test SR: ', y_test.sum()/y_test.count())\n",
    "print('Validation SR: ', y_val.sum()/y_val.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imputation median\n",
    "imp=Imputer(missing_values=\"NaN\", strategy=\"median\", axis=0) #specify axis\n",
    "imp.fit(X_train)\n",
    "X_train_imputed_df = pd.DataFrame(imp.transform(X_train), columns = X.columns)\n",
    "X_test_imputed_df = pd.DataFrame(imp.transform(X_test), columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#standardisation\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X_train_imputed_df)\n",
    "X_train_std = pd.DataFrame(scaler.transform(X_train_imputed_df), columns = X.columns)      \n",
    "X_test_std = pd.DataFrame(scaler.transform(X_test_imputed_df), columns = X.columns)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature selection\n",
    "Feature selection is a process where features that contribute most to the prediction are automatically selected.\n",
    "\n",
    "Having too many irrelevant features in data can decrease the accuracy of the models. Three benefits of performing feature selection before training models are:\n",
    "\n",
    "- Reduced Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n",
    "- Improved Accuracy: Less misleading data means modeling accuracy improves.\n",
    "- Reduced Training Time: Less data means that algorithms train faster.\n",
    "\n",
    "Two different feature selection methods provided by the scikit-learn Python library are Recursive Feature Elimination and feature importance ranking.\n",
    "The Univariate feature selection - chi2 - best 35 was tested but got better results with Recursive feature elimination - best 35. Beforehand, dummy features with less than 1% occurence were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removing features with less than 1% of occurence\n",
    "cat_occurence = pd.DataFrame({'occurence':dummies.sum()/dummies.shape[0]})\n",
    "dummies_to_drop = cat_occurence[cat_occurence['occurence'] < 0.01].index\n",
    "X_train = X_train.drop(dummies_to_drop, axis=1)\n",
    "X_train_std = X_train_std.drop(dummies_to_drop, axis=1)\n",
    "X_train_imputed_df = X_train_imputed_df.drop(dummies_to_drop, axis=1)\n",
    "X_test = X_test.drop(dummies_to_drop, axis=1)\n",
    "X_test_std = X_test_std.drop(dummies_to_drop, axis=1)\n",
    "X_test_imputed_df = X_test_imputed_df.drop(dummies_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Recursive Feature Elimination (RFE) is based on the idea to repeatedly construct a model \n",
    "#and choose either the best or worst performing feature, \n",
    "#setting the feature aside and then repeating the process with the rest of the features. \n",
    "#This process is applied until all features in the dataset are exhausted. \n",
    "#The goal of RFE is to select features by recursively considering smaller and smaller sets of features.\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "rfe = RFE(logreg, 35)\n",
    "rfe = rfe.fit(X_train_std, y_train)\n",
    "rfe_support = pd.DataFrame({'rfe_support':rfe.support_, 'features':X_train_std.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features_names\n",
    "X_train_imputed_df = X_train_imputed_df[rfe_support[rfe_support['rfe_support'] == True].features.values]\n",
    "X_train_std = X_train_std[rfe_support[rfe_support['rfe_support'] == True].features.values]\n",
    "X_test_imputed_df = X_test_imputed_df[rfe_support[rfe_support['rfe_support'] == True].features.values]\n",
    "X_test_std = X_test_std[rfe_support[rfe_support['rfe_support'] == True].features.values]\n",
    "features_names = rfe_support[rfe_support['rfe_support'] == True].features.values\n",
    "features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Because of the Imbalanced dataset (way more 0 than 1), SMOTE (Synthetic Minority Over-sampling Technique) is performed.\n",
    "RANDOM_STATE = 0 \n",
    "from imblearn import over_sampling as os\n",
    "sm = os.SMOTE(random_state=RANDOM_STATE)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train_std, y_train)\n",
    "X_train_imputed_res, y_train_imputed_res = sm.fit_sample(X_train_imputed_df, y_train)\n",
    "X_train_imputed_res = pd.DataFrame(X_train_imputed_res, \n",
    "                                   columns = X_train_imputed_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Decision Tree Algorithm\n",
    "Advantages:\n",
    "    - simple to understand and interpret. \n",
    "    - Have value even with little hard data. Important insights can be generated based on experts describing a situation (its alternatives, probabilities, and costs) and their preferences for outcomes.\n",
    "    - Help determine worst, best and expected values for different scenarios.\n",
    "    - Can be combined with other decision techniques.\n",
    "Disadvantages :\n",
    "    - unstable. A small change in the data can lead to a large change in the struture of the optimal decision tree.\n",
    "    - often relatively inaccurate. Many other predictors perform better with similar data. This can be remedied by replacing a single decision tree with a random forest of decision trees, but a random forest is not as easy to interpret as a single decision tree.\n",
    "\n",
    "For data including categorical variables with different number of levels, information gain in decision trees is biased in favor of those attributes with more levels.\n",
    "Calculations can get very complex, particularly if many values are uncertain and/or if many outcomes are linked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn import over_sampling as os\n",
    "from imblearn import pipeline as pl\n",
    "from imblearn.metrics import (geometric_mean_score,\n",
    "                              make_index_balanced_accuracy)\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, SelectPercentile\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error,confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "RANDOM_STATE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_tree = tree.DecisionTreeClassifier(max_depth=4,\n",
    "                                  min_samples_leaf=5000)\n",
    "clf_tree.fit(X_train_imputed_df, y_train)\n",
    "tree.export_graphviz(clf_tree, \n",
    "                     out_file='tree_all.dot',\n",
    "                     feature_names=X_train_imputed_df.columns, \n",
    "                     proportion = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create Pipeline with SMOTE + Classifier\n",
    "    - Perceptron\n",
    "    - Linear Regression\n",
    "    - Logistic Regression\n",
    "    - Lasso\n",
    "    - Decision Tree\n",
    "    - Linear Regression (LinR)- ok\n",
    "    - Logistic Regression (LogR)- ok\n",
    "    - Lasso Regression (LassoR)- ok \n",
    "    - Decision Tree (DT)- ok\n",
    "    - Random Forest (RF) - ok\n",
    "    - K-Nearest Neighbor (KNN) - ok\n",
    "    - Support Vector Machine (SVM - not ok. very costly. feature selection necessary)\n",
    "    - XG Boost\n",
    "    - Naïve Bayesian Classifier (BC)\n",
    "    - Bayesian Network (BN)\n",
    "    - Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import calibration_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn import over_sampling as os\n",
    "from imblearn import pipeline as pl\n",
    "from imblearn.metrics import (geometric_mean_score,\n",
    "                              make_index_balanced_accuracy)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from itertools import cycle\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "# #############################################################################\n",
    "# Measure Prediction Score\n",
    "\n",
    "def score_prediction(y_pred, y_test):\n",
    "    print(\"accuracy score:   %0.3f\" %  metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"classification report:\", metrics.classification_report(y_test, y_pred, digits=3))\n",
    "    print(\"confusion matrix:\")\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print('Misclassified samples: %d' %(y_test != y_pred).sum())\n",
    "    print('Recall score:', recall_score(y_test, y_pred))\n",
    "    print('Coverage:', (cm[0,1]+cm[1,1])/cm.sum())\n",
    "    # ---- ROC Curve -----  \n",
    "    #probs = pipeline.predict_proba(X_test_std)\n",
    "    preds = y_pred \n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Benchmark classifiers\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    # ---- time ----- \n",
    "    t0 = time()\n",
    "    pipeline = pl.make_pipeline(os.SMOTE(random_state=RANDOM_STATE),\n",
    "                                clf)\n",
    "    pipeline.fit(X_train_std, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "    t0 = time()\n",
    "    y_pred = pipeline.predict(X_test_std)\n",
    "    #y_pred = np.where(y_pred>0.1, 1, 0)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "    \n",
    "    score_prediction(y_pred, y_test)\n",
    "    # ---- Mode description -----  \n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, train_time, test_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for clf, name in (\n",
    "    #(DummyClassifier(strategy='most_frequent',random_state=0), \"Dummy Classifier\"),\n",
    "    (Perceptron(n_iter=50, random_state=0), \"Perceptron\"),\n",
    "    (linear_model.SGDClassifier(random_state=0), \"Linear SGD\"),\n",
    "    (linear_model.LogisticRegression(penalty='l1',random_state=0), \"Logistic + L1\"),\n",
    "    (linear_model.LogisticRegression(penalty='l2',random_state=0), \"Logistic + L2\"),\n",
    "    (RidgeClassifier(tol=1e-2, solver=\"lsqr\", random_state=0), \"Ridge Classifier\"),\n",
    "    (PassiveAggressiveClassifier(n_iter=50, random_state=0), \"Passive-Aggressive\"),\n",
    "    (KNeighborsClassifier(n_neighbors=10), \"kNN\"), #too long on all features : train time: 205.479s, test time:  26099.829s\n",
    "    (tree.DecisionTreeClassifier(max_depth=4, min_samples_leaf=5000, random_state=0), \"Decision Tree\"),\n",
    "    (RandomForestClassifier(n_estimators=100, random_state=0), \"Random forest\"),\n",
    "    (RandomForestClassifier(n_estimators=100, random_state=0, max_depth=4, min_samples_leaf=5000), \"Random forest 2\"),\n",
    "    (RandomForestClassifier(criterion = 'gini',n_estimators=400,random_state=RANDOM_STATE,\n",
    "                            n_jobs=2, min_samples_leaf=8000, max_features = 10), \"Random forest 3\"),\n",
    "    (LinearSVC(C=1.0), \"Linear SVC\"),\n",
    "    (SGDClassifier(alpha=.0001, n_iter=50, penalty=\"elasticnet\"), 'Elastic-Net penalty'),\n",
    "    (NearestCentroid(), 'NearestCentroid (aka Rocchio classifier)'),\n",
    "    (GaussianNB(), 'Gaussian Naive Bayes'),\n",
    "    (BernoulliNB(alpha=.01), 'Bernouilli Naive Bayes')\n",
    "    \n",
    "#                                       \n",
    "):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    benchmark(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Fine-tuning Random Forest via grid-search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=20, class_weight={0: 1, 1: 3000})\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "scoring = {'precision': 'precision', 'Recall': 'recall', 'accuracy': 'accuracy', 'AP':'average_precision' }\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, \n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, scoring = scoring,\n",
    "                                   refit='precision' \n",
    "                                  )\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X_train_imputed_df, y_train)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "#report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "argmax = (random_search.cv_results_['mean_test_precision']).argmax()\n",
    "recall = random_search.cv_results_['mean_test_Recall'][argmax]\n",
    "average_precision  = random_search.cv_results_['mean_test_AP'][argmax]\n",
    "precision  = random_search.cv_results_['mean_test_precision'][argmax]\n",
    "\n",
    "print(argmax, recall,average_precision,  precision)\n",
    "print(random_search.cv_results_['params'][argmax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Combination of classifiers:\n",
    "1. Logistic L1 (linear_model.LogisticRegression(penalty='l1',random_state=0), \"Logistic + L1\")\n",
    "2. RF 5000 (RandomForestClassifier(criterion = 'entropy',n_estimators=400,random_state=RANDOM_STATE, n_jobs=2, min_samples_leaf=5000, max_features = 10), \"Random forest 2\")\n",
    "3. Nearest Centroid NearestCentroid()\n",
    "4. Bernouilli BernoulliNB(alpha=.01)\n",
    "5. Decision Tree (tree.DecisionTreeClassifier(max_depth=4, min_samples_leaf=5000, random_state=0), \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "clf1 = linear_model.LogisticRegression(random_state=1, penalty='l1')\n",
    "clf2 = RandomForestClassifier(criterion = 'entropy',n_estimators=400,random_state=RANDOM_STATE, n_jobs=2, min_samples_leaf=5000, max_features = 10)\n",
    "clf3 = NearestCentroid()\n",
    "clf4 = BernoulliNB(alpha=.01)\n",
    "clf5 = tree.DecisionTreeClassifier(max_depth=4, min_samples_leaf=5000, random_state=0)\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=[\n",
    "        ('lr', clf1), ('rf', clf2), ('nc', clf3), ('nb', clf4), ('dt', clf5)], voting='hard')\n",
    "\n",
    "pipe1 = pl.make_pipeline(os.SMOTE(random_state=RANDOM_STATE), eclf1)\n",
    "\n",
    "eclf1 = pipe1.fit(X_train_std, y_train)\n",
    "print(eclf1.predict(X_train_std))\n",
    "\n",
    "eclf2 = VotingClassifier(estimators=[\n",
    "        ('lr', clf1), ('rf', clf2), ('dt', clf5)],\n",
    "        voting='soft')\n",
    "\n",
    "pipe2 = pl.make_pipeline(os.SMOTE(random_state=RANDOM_STATE), eclf2)\n",
    "\n",
    "eclf2 = pipe2.fit(X_train_std, y_train)\n",
    "print(eclf2.predict(X_test_std))\n",
    "\n",
    "eclf3 = VotingClassifier(estimators=[\n",
    "       ('lr', clf1), ('rf', clf2), ('nc', clf3), ('nb', clf4), ('dt', clf5)],\n",
    "       voting='hard', weights=[2,3,1,1,1],\n",
    "       flatten_transform=True)\n",
    "\n",
    "pipe3 = pl.make_pipeline(os.SMOTE(random_state=RANDOM_STATE), eclf3)\n",
    "\n",
    "eclf3 = pipe3.fit(X_train_std, y_train)\n",
    "eclf4 = VotingClassifier(estimators=[\n",
    "        ('lr', clf1), ('rf', clf2), ('dt', clf5)],\n",
    "        voting='hard')\n",
    "\n",
    "pipe4 = pl.make_pipeline(os.SMOTE(random_state=RANDOM_STATE), eclf4)\n",
    "\n",
    "eclf4 = pipe4.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Ensemble 1')\n",
    "y_pred_1 = eclf1.predict(X_test_std)\n",
    "print(score_prediction(y_pred_1, y_test))\n",
    "print('Ensemble 2')\n",
    "y_pred_2 = eclf2.predict(X_test_std)\n",
    "print(score_prediction(y_pred_2, y_test))\n",
    "print('Ensemble 3')\n",
    "y_pred_3 = eclf3.predict(X_test_std)\n",
    "print(score_prediction(y_pred_3, y_test))\n",
    "print('Ensemble 4')\n",
    "y_pred_4 = eclf4.predict(X_test_std)\n",
    "print(score_prediction(y_pred_4, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all the stuff here that is not necessary for supporting the points above. Good place for documentation without distraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. TO DO      \n",
    "    - k fold CV\n",
    "    - Gradient Boosting\n",
    "    - Support Vector Machine (SVM - not ok. very costly. feature selection necessary)\n",
    "    - XG Boost\n",
    "    - Naïve Bayesian Classifier (BC) \n",
    "    - gnb = GaussianNB()\n",
    "    - Bayesian Network (BN)\n",
    "https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}